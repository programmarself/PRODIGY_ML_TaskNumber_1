# -*- coding: utf-8 -*-
"""House_Price_Predictions.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Jx9VjBJl9OIBGTb_Wcj-2IRczO2b7Y2r

# **House Prices Prediction**


---

<h1 style="font-family: 'poppins'; font-weight: bold; color: Green;">ðŸ‘¨ðŸ’»Author: Irfan Ullah Khan</h1>

[![GitHub](https://img.shields.io/badge/GitHub-Profile-blue?style=for-the-badge&logo=github)](https://github.com/programmarself)
[![Kaggle](https://img.shields.io/badge/Kaggle-Profile-blue?style=for-the-badge&logo=kaggle)](https://www.kaggle.com/programmarself)
[![LinkedIn](https://img.shields.io/badge/LinkedIn-Profile-blue?style=for-the-badge&logo=linkedin)](https://www.linkedin.com/in/irfan-ullah-khan-4a2871208/)  

[![YouTube](https://img.shields.io/badge/YouTube-Profile-red?style=for-the-badge&logo=youtube)](https://www.youtube.com/@irfanullahkhan7748)
[![Email](https://img.shields.io/badge/Email-Contact%20Me-red?style=for-the-badge&logo=email)](mailto:programmarself@gmail.com)
[![Website](https://img.shields.io/badge/Website-Contact%20Me-red?style=for-the-badge&logo=website)](https://datasciencetoyou.odoo.com)

![](https://i.imgur.com/3sw1fY9.jpg)

<div class="alert alert-block alert-info" style='color:black;'>
    <h2>Introduction</h2>
   Predicting the selling price of a home is an important topic in real estate. There are several elements that influence the price of a home. Some factors may produce an increase in price, others may cause a fall, and yet others are reliant on one or more factors, i.e. their combination with other factors determines whether they will increase or decrease the price. To assist us in determining the relationship between these attributes and sale prices, we have data from 1460 residences (sold). The dataset covers nearly all of the elements that influence a house's sales price, including overall quality, neighborhood, the presence of a basement and/or garage, and so on, in addition to the sale price.
    The aim is to, perform exploratory data analysis for finding out which factors affect the most. We will be using of multiple
    machine learning algorithms and choose the one which has the highest accuracy. Then training, evaluating and tuning the model with
    appropriate parameter values, we will try to keep the RMSE minimum. I will also create a web application having user frinedly
    interfece where one can easily get the sale price of their house just by giving the different attributes of the house as an input.
</div>
"""

#filtering the warnings to keep the notebook clean
import warnings
warnings.filterwarnings('ignore')

"""# 1. Loading and Exploring Data"""

import pandas as pd
import numpy as np
#showing max rows
pd.options.display.max_columns = 200
pd.options.display.max_rows = 200

prices_df = pd.read_csv('/content/train.csv')
prices_df.head()

prices_df.shape

"""Let's explore the columns and data types within the dataset."""

prices_df.info()

#Statistical Information of the Numeric Columns with Transpose method
prices_df.describe().T

#Statistical Information of the Categorical Columns
prices_df.describe(include=['O'])

"""#### Visualizing the dataset"""

# Commented out IPython magic to ensure Python compatibility.
#importing the dependencies for visualization
import plotly.express as px
import matplotlib
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

#setting the style and background
sns.set_style('darkgrid')
matplotlib.rcParams['font.size'] = 14
matplotlib.rcParams['figure.figsize'] = (10, 6)
matplotlib.rcParams['figure.facecolor'] = '#00000000'

"""#### Sales Prices"""

px.histogram(prices_df, x='SalePrice', title='Distribution of Sales Price', color='SaleCondition')

px.histogram(x=np.log(prices_df['SalePrice']), title='Logarithmic Distribution of Sales Price').update_layout(
    xaxis_title="Logarithmic SalePrice")

"""<div class="alert alert-block alert-info" style='color:black;'>
    Majority of the data points lie in the range of 100k-300k and as we move further the number of expensive properties sold
    decreases. However the logarithmic distribution of sale prices appears to be normal.
</div>

#### Year Built and Year Sold
"""

fig, axes = plt.subplots(2, 1, figsize=(12, 8))
axes[0].set_title('Sales Prices vs Year Sold/Built')

xdmd=prices_df.groupby(['YrSold', 'MoSold'], as_index=False)['SalePrice'].median()
axes[0].plot(xdmd.index, xdmd.SalePrice)
axes[0].set_xlabel('Year Sold')
axes[0].set_ylabel('Sale Price')
axes[0].set_xticks([5.5, 17.5, 29.5, 41.5, 53.5])
axes[0].set_xticklabels([2006, 2007, 2008, 2009, 2010])


xdmdb=prices_df.groupby('YearBuilt')['SalePrice'].median()
axes[1].plot(xdmdb.index,xdmdb)
axes[1].set_xlabel('Year Built')
axes[1].set_ylabel('Sale Price');

"""<div class="alert alert-block alert-info" style='color:black;'>
    1. The sales prices are seem to be increasing from the end of the 2006 to till end of the 2007 with the a spike in Dec, 2022. Then
    they show sudden decrease in Jan 2008 and do not show much variation in the prices afterwards. The reasons behind this sudden fall
    were low interest rates, easy credit, insufficient regulation, and toxic subprime mortgages. You can read more about it over here
    : https://financialcomplete.com/why-did-house-prices-fall-in-2008. <br>
    2. After 1925, the sale price appears to be propotional to the year when the house was built i.e. old houses are sold for the
    lesser price than the new houses. The reason behind this could be the condition of houses getting bad over the period of time.
</div>

#### Overall Quality of the Houses
"""

fig, axes = plt.subplots(1,2, figsize=(20, 5))
axes[0].set_title('OverallQuality (Boxplot)')
axes[0].set_xticklabels(labels=prices_df['OverallQual'].unique().tolist())
sns.boxplot(y=prices_df['SalePrice'], x=prices_df['OverallQual'], ax=axes[0])
axes[1].set_title('OverallQuality (Barplot)')
sns.countplot(x=prices_df['OverallQual'], ax=axes[1]);

"""<div class="alert alert-block alert-info" style='color:black;'>
    Sale price is increasing with over all quality which is no surprise. However most houses sold have average (5-6) overall
    condition.
</div>

#### Quality, Condition and Year Built
"""

px.scatter_3d(prices_df, x='OverallQual', y='OverallCond', z='YearBuilt',
              title='Relation between Quality, Condition and Year Built')

"""<div class="alert alert-block alert-info" style='color:black;'>
    1. The relationship between over all quality and over all condition isn't linear (as someone might expect). It appears that houses
    can be in good condition even though their over all quality is poor.<br>
    2. The over all quality of house built between 1900 to 1960, falls below average whereas it's above average for houses built after
    1960. And it's average for houses built before 1900.<br>
    3. The houses having over all condition below average were mostly built between 1900-1980.
</div>

#### Overall Condition of the Houses
"""

fig, axes = plt.subplots(1,2, figsize=(20, 5))
axes[0].set_title('OverallCond (Boxplot)')
axes[0].set_xticklabels(labels=prices_df['OverallCond'].unique().tolist())
sns.boxplot(y=prices_df['SalePrice'], x=prices_df['OverallCond'], ax=axes[0])
axes[1].set_title('OverallCond (Barplot)')
sns.countplot(x=prices_df['OverallCond'], ax=axes[1]);

"""<div class="alert alert-block alert-info" style='color:black;'>
    It doesn't show linear increment in sale price with the over all condtion as we have already seen that good condition doesn't
    imply good quality (and this is the reason sales price isn't significantly increasing with overall condition rating getting
    increased from 6 to 7 till 8).  The above graphs shows large variation in the sale price of the houses having average over all
    condition and also number of houses sold in that category are larger.
</div>

#### Streets and Allies
"""

xdf=prices_df.groupby(['Street','Alley','MSZoning','Neighborhood'], as_index=False, dropna=False)['Id'].count()
xdf.fillna('NA', inplace=True)
xdf.rename(columns={'Id':'Number of Houses'}, inplace=True)
px.sunburst(xdf,path=['Street','Alley','MSZoning','Neighborhood'], values='Number of Houses', color='Number of Houses')

"""<div class="alert alert-block alert-info" style='color:black;'>
    The above sunburst classifies the number of houses based on the `Street, Alley, MSZoning` and `Neighbourhood`. We can draw mainly
    two conclusions from this.<br>
    1. Almost all the streets are `Paved`(only six are `Gravel`) <br>
    2. Majority of the houses don't have alley access (indicated by `NA`)
</div>

#### Neighbourhood
"""

fig, axes = plt.subplots(1,2, figsize=(20, 5))
axes[0].set_title('Neighborhood (Boxplot)')
axes[0].set_xticklabels(labels=prices_df['Neighborhood'].unique().tolist(),rotation=90)
sns.boxplot(y=prices_df['SalePrice'], x=prices_df['Neighborhood'], ax=axes[0])
axes[1].set_title('Neighborhood (Barplot)')
sns.countplot(x=prices_df['Neighborhood'], ax=axes[1])
plt.xticks(rotation=90);

"""<div class="alert alert-block alert-info" style='color:black;'>
    The houses from neighbourhoods `NridgHt (Northridge Heights)` and `StoneBr (Stone Brook)` show large variation in the `SalePrice`.
    The `SalePrice` is in the `NoRidge (Northridge)` neighbourhood. However the maximum numbers of houses are sold in `NAmes (North
    Ames)` neighbourhood followed by `CollgCr (College Creek)` and the least number of houses are sold in `Blueste (Bluestem)`
    followed by `NPkVill (Northpark Villa)`. You may notice that the sale prices from `NAmes` neighbourhood do not vary much and are
    below average that could be the reason they are among the most sold houses.
</div>

##### Let's explore the North Ames neighbourhood some more
"""

NAmes_df=prices_df[prices_df['Neighborhood']=='NAmes']
NAmes_df.head()

NAmes_df['SalePrice'].median(), prices_df['SalePrice'].median()

NAmes_OverallQual=NAmes_df['OverallQual'].value_counts()/len(NAmes_df['OverallQual'])
plt.pie(NAmes_OverallQual, autopct='%f')
plt.legend(NAmes_OverallQual.index, loc=(1,0.5))
plt.title('Overall Quality of the Houses from NAmes Neighbourhood');

"""<div class="alert alert-block alert-info" style='color:black;'>
    See that the average sale price in the `NAmes` neighbourhood is below average of the whole dataset. Most of the houses (nearly
    82%) have average(5-6) overall quality that's what makes them affordable.
</div>

##### We can similiary use loop through all the other categorical variables and discrete variables to obtain their relationship with `SalePrice` using boxplot.
"""

discrete = ['MSSubClass','BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath','BedroomAbvGr', 'KitchenAbvGr',
'TotRmsAbvGrd', 'Fireplaces','GarageCars', 'MoSold','YrSold']
categorical=['MSZoning', 'Street', 'Alley', 'LotShape', 'LandContour', 'Utilities',
 'LotConfig', 'LandSlope', 'Condition1', 'Condition2','HouseStyle', 'RoofStyle',
 'RoofMatl', 'Exterior1st', 'Exterior2nd','BldgType','MasVnrType', 'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual',
 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'Heating', 'HeatingQC', 'CentralAir', 'Electrical',
 'KitchenQual', 'Functional', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'PavedDrive',
 'PoolQC', 'Fence', 'MiscFeature', 'SaleType', 'SaleCondition']

for i in discrete+categorical:
    fig, axes = plt.subplots(1,2, figsize=(20, 5))
    axes[0].set_title(i+' (Boxplot)')
    sns.boxplot(y=prices_df['SalePrice'], x=prices_df[i], ax=axes[0])
    axes[1].set_title(i+' (Barplot)')
    sns.countplot(x=prices_df[i], ax=axes[1])

"""<div class="alert alert-block alert-info" style='color:black;'>
    Here the countplot shows number of houses sold corresponding to the value of the attribute and the boxplot shows distribution of
    `SalePrice` corresponding to the value of the attribute. Observe that the number of houses sold corresponding to a value of the
    attribute can be less in number and still can cause wide distribution of corresponding `SalePrice`.
</div>

##### Now will find the distribution of numeric values while comparing the same attribute with SalePrice using scatterplot
"""

Numeric_Attributes=['LotFrontage','LotArea','YearBuilt',
 'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF',
 'LowQualFinSF', 'GrLivArea',   'GarageYrBlt', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch',
 '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal']

for i in Numeric_Attributes:
    fig, axes = plt.subplots(1,2, figsize=(15, 5))
    axes[0].set_title(i+' (Distplot)')
    sns.distplot(prices_df[i], ax=axes[0])
    axes[1].set_title(i+' (Scatterplot)')
    sns.scatterplot(y=prices_df['SalePrice'], x=prices_df[i], ax=axes[1])

"""<div class="alert alert-block alert-info" style='color:black;'>
    From the displot one can conclude that the numeric attributes aren't normally distributed also they have wide range. We need to
    deal with the outliers that cause this wide distribution of the data.
</div>

#### Correlation between the Attributes
Lighter, more positively correlated <br>
Darker, more negatively correlated
"""

prices_df.corr()

plt.figure(figsize=(10,10))
sns.heatmap(prices_df.corr().sort_values(by='SalePrice',ascending=False))
plt.title('Correlation of the Attributes');

"""<div class="alert alert-block alert-info" style='color:black;'>
   The column names on the left side of the heatmap are arranged in descending order based on their correlation with the SalePrice, which is displayed at the extreme right of the heatmap. This visualization highlights the factors that have the strongest influence on the SalePrice, as indicated by their high correlation values. It is evident from this heatmap that the overall condition of the house plays a more significant role in determining its sale price compared to other factors.
</div>

# 2. Preparing the Dataset for Training

Before we can train the model, we need to prepare the dataset. Here are the steps we'll follow:

1. Identify the columns that show wide distribution of the ranges (Use previous distribution plots)
2. Identify the input and target column(s) for training the model.
2. Identify numeric and categorical input columns.
4. Deal with outliers by taking the logs of the widely spread attributes.
3. [Impute](https://scikit-learn.org/stable/modules/impute.html) (fill) missing values in numeric columns
4. [Scale](https://scikit-learn.org/stable/modules/preprocessing.html#scaling-features-to-a-range) values in numeric columns to a (0,1) range.
5. [Encode](https://scikit-learn.org/stable/modules/preprocessing.html#encoding-categorical-features) categorical data.
6. Split the dataset into training and validation sets.

### Identify and Substitute the widely distributed columns
Here we will take all the numeric columns we have used for plotting above
"""

import numpy as np
log_numeric_cols=Numeric_Attributes.copy()
prices_df[log_numeric_cols]=np.log(prices_df[log_numeric_cols]+1)

prices_df[log_numeric_cols].head()

"""### Identify Inputs and Targets

While the dataset contains 81 columns, not all of them are useful for modeling. Note the following:

- The first column `Id` is a unique ID for each house and isn't useful for training the model.
- The last column `SalePrice` contains the value we need to predict i.e. it's the target column.
- Data from all the other columns (except the first and the last column) can be used as inputs to the model.

"""

# Identify the input columns (a list of column names)
input_cols = ['MSSubClass', 'MSZoning', 'LotFrontage', 'LotArea', 'Street',
       'Alley', 'LotShape', 'LandContour', 'Utilities', 'LotConfig',
       'LandSlope', 'Neighborhood', 'Condition1', 'Condition2', 'BldgType',
       'HouseStyle', 'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd',
       'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType',
       'MasVnrArea', 'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual',
       'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinSF1',
       'BsmtFinType2', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'Heating',
       'HeatingQC', 'CentralAir', 'Electrical', '1stFlrSF', '2ndFlrSF',
       'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath',
       'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'KitchenQual',
       'TotRmsAbvGrd', 'Functional', 'Fireplaces', 'FireplaceQu', 'GarageType',
       'GarageYrBlt', 'GarageFinish', 'GarageCars', 'GarageArea', 'GarageQual',
       'GarageCond', 'PavedDrive', 'WoodDeckSF', 'OpenPorchSF',
       'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'PoolQC',
       'Fence', 'MiscFeature', 'MiscVal', 'MoSold', 'YrSold', 'SaleType',
       'SaleCondition']

# Identify the name of the target column (a single string, not a list)
target_col = 'SalePrice'

"""Make sure that the `Id` and `SalePrice` columns are not included in `input_cols`.

Now that we've identified the input and target columns, we can separate input & target data.
"""

inputs_df = prices_df[input_cols].copy()

targets = prices_df[target_col]

"""### Identify Numeric and Categorical Data

The next step in data preparation is to identify numeric and categorical columns. We can do this by looking at the data type of each column.
"""

#getting the list of numeric columns
numeric_cols = inputs_df.select_dtypes(include=['int64', 'float64']).columns.tolist()

#getting the list of categorical columns
categorical_cols = inputs_df.select_dtypes('object').columns.tolist()

prices_df[numeric_cols].head()

"""### Impute Numerical Data

Some of the numeric columns in our dataset contain missing values (`nan`).
"""

#We will fill the missing values only from the numeric columns
#Categorical columns can be dealt with using encoder
missing_counts = inputs_df[numeric_cols].isna().sum().sort_values(ascending=False)
missing_counts[missing_counts > 0]

"""There are several techniques for imputation, but we'll use the most basic one: replacing missing values with the average value in the column using the `SimpleImputer` class from `sklearn.impute`."""

from sklearn.impute import SimpleImputer

# 1. Create the imputer
imputer = SimpleImputer(strategy = 'median')

# 2. Fit the imputer to the numeric colums
imputer.fit(prices_df[numeric_cols])

# 3. Transform and replace the numeric columns
inputs_df[numeric_cols] = imputer.transform(inputs_df[numeric_cols])

"""After imputation, none of the numeric columns should contain any missing values."""

missing_counts = inputs_df[numeric_cols].isna().sum().sort_values(ascending=False)
missing_counts[missing_counts > 0] # should be an empty list

"""### Scaling Numerical Data
A good practice is to scale numeric features to a small range of values e.g. $(0,1)$. Scaling numeric features ensures that no particular feature has a disproportionate impact on the model's loss. Optimization algorithms also work better in practice with smaller numbers.
"""

inputs_df[numeric_cols].describe().loc[['min', 'max']]

from sklearn.preprocessing import MinMaxScaler

# Create the scaler
scaler = MinMaxScaler()

# Fit the scaler to the numeric columns
scaler.fit(prices_df[numeric_cols])

# Transform and replace the numeric columns
inputs_df[numeric_cols] = scaler.transform(inputs_df[numeric_cols])

inputs_df[numeric_cols].describe().loc[['min', 'max']]

"""All ranges have been scaled to 0 to 1

### Encode Categorical Columns

Our dataset contains several categorical columns, each with a different number of categories.
"""

inputs_df[categorical_cols].nunique().sort_values(ascending=False)

"""

Since machine learning models can only be trained with numeric data, we need to convert categorical data to numbers. A common technique is to use one-hot encoding for categorical columns.
One hot encoding involves adding a new binary (0/1) column for each unique category of a categorical column."""

from sklearn.preprocessing import OneHotEncoder

# 1. Create the encoder
encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')

# 2. Fit the encoder to the categorical colums
encoder.fit(prices_df[categorical_cols])

# 3. Generate column names for each category
encoded_cols = list(encoder.get_feature_names_out(categorical_cols))
print(len(encoded_cols))

# 4. Transform and add new one-hot category columns
inputs_df[encoded_cols] = encoder.transform(inputs_df[categorical_cols])

"""The new one-hot category columns should now be added to `inputs_df`."""

inputs_df.head()

"""But we only need imputed and sclaed numeric columns along with encoded categorical columns"""

inputs_df[numeric_cols + encoded_cols].head()

"""##### We will also take log of the target column to scale it down"""

log_targets = np.log(targets)

"""# 3. K-Fold Cross Validation
We will use K-Fold Cross Validation for choosing the best model for predictions and tune that model only.
"""

#Importing Important Functions
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score

#Importing models
from sklearn.linear_model import LinearRegression
from sklearn.linear_model import Ridge
from sklearn.linear_model import Lasso
from sklearn.linear_model import ElasticNet
from sklearn.linear_model import SGDRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.svm import SVR
from xgboost import XGBRegressor

#Creating the list of models for cross validation
models=[
    ('Rg',Ridge()),
    ('Ls',Lasso()),
    ('EN',ElasticNet()),
    ('SGD',SGDRegressor()),
    ('DT',DecisionTreeRegressor()),
    ('RF',RandomForestRegressor()),
    ('GB',GradientBoostingRegressor()),
    ('SVR',SVR()),
    ('XGB',XGBRegressor())
]

#Evaluae the created models
results=[]
names=[]
for name,model in models:
    kfold=KFold(n_splits=20)
    cv_results=cross_val_score(model, inputs_df[numeric_cols + encoded_cols], log_targets, cv=kfold)
    results.append(cv_results)
    names.append(name)
    print('%s: %f (%f)' %(name,cv_results.mean(),cv_results.std()))

#Compare our models
plt.boxplot(results,labels=names)
plt.title("Algorithm Comparison")
plt.show()

"""From the above plot one can conclude that the `GradientBoostingRegressor()` and `Ridge()` are the best performing models.

# 4. Training, Evaluating and Tuning
Now we will train and tune the best model we have found out here.

#### Training and Validation Set

Finally, let's split the dataset into a training and validation set. We'll use a randomly select 25% subset of the data for validation. Also, we'll use just the numeric and encoded columns, since the inputs to our model must be numbers.
"""

from sklearn.model_selection import train_test_split

train_inputs, val_inputs, train_targets, val_targets = train_test_split(inputs_df[numeric_cols + encoded_cols],
                                                                        log_targets,
                                                                        test_size=0.25,
                                                                        random_state=42)

"""##### We will train both the models simultaneously and compare their accuracies
Note: 1 refers to `Ridge()` and 2 refers to `GradientBoostingRegressor()`.
"""

# Create the model
model1 = Ridge()
model2 = GradientBoostingRegressor()

# Fit the model using inputs and targets
model1.fit(train_inputs, train_targets)
model2.fit(train_inputs, train_targets)

"""The model is now trained, and we can use it to generate predictions for the training and validation inputs. We can evaluate the model's performance using the RMSE (root mean squared error) loss function. This is the same function used over the logs of the values predicted while checking the accuracy of the submissions."""

# Defininf the RMSE function
from sklearn.metrics import mean_squared_error
def rmse(preds, targets):
    return mean_squared_error(preds, targets, squared = False)

#Predictions over train data and checking the RMSE
train_preds1 = model1.predict(train_inputs)
train_preds2 = model2.predict(train_inputs)
train_rmse1 = rmse(train_targets, train_preds1)
train_rmse2 = rmse(train_targets, train_preds2)
print('The RMSE loss for the training set 1 is {}.'.format(train_rmse1))
print('The RMSE loss for the training set 2 is {}.'.format(train_rmse2))

#Predictions over validation data and checking the RMSE
val_preds1 = model1.predict(val_inputs)
val_preds2 = model2.predict(val_inputs)
val_rmse1 = rmse(val_targets, val_preds1)
val_rmse2 = rmse(val_targets, val_preds2)
print('The RMSE loss for the validation set 1 is {}.'.format(val_rmse1))
print('The RMSE loss for the validation set 2 is {}.'.format(val_rmse2))

"""The `Ridge()` has higher training error but lower validation error than `GradientBoostingRegressor()` which indicates that the `Ridge()` is well generalized. But Gradient Boosting Algorithm had given better accuracy during the process of KFold Cross Validation so we won't rule it out yet, we will keep going with both models and see what the end results are."""

fig, axes = plt.subplots(2,2, figsize=(20, 10))

axes[0,0].set_title('Ridge')
sns.regplot(x=train_targets, y=train_preds1, ax=axes[0,0])
axes[0,0].set_ylabel('Predicted Sale Price for Train')

axes[0,1].set_title('Gradient Boosting Regressor')
sns.regplot(x=train_targets, y=train_preds2, ax=axes[0,1])
axes[0,1].set_ylabel('Predicted Sale Price for Train')

sns.regplot(x=val_targets, y=val_preds1, ax=axes[1,0])
axes[1,0].set_ylabel('Predicted Sale Price for Validation')

sns.regplot(x=val_targets, y=val_preds2, ax=axes[1,1])
axes[1,1].set_ylabel('Predicted Sale Price for Validation');

"""The predictions given by `Ridge()` seem to be less deviated from the the actual values, but `GradientBoostingRegressor()` hasn't done a complete wrong job, there are few data points which causing a bit higher RMSE.

#### Hyperparamter Tuning
This process involves finding the proper values of the different parameters so that the model is well generlized i.e. makes predictions on the validation data with better accuracy, even if it comes at the cost of increment in RMSE of training data.

##### Ridge
We will manually choose different values of parameters and see which one gives the less RMSE for validation data. We will loop through list of parameter values and keep varying the list till we get the most accurate value (till 4rth decimal).
"""

#parameter_values=[i/10 for i in range(1,11)] gives 0.2
#parameter_values=[i/100 for i in range(10,20)] gives 0.15
#parameter_values=[i/1000 for i in range(140,160)] gives 0.147
parameter_values=[i/10000 for i in range(1460,1480)]
train_errors=[]
val_errors=[]
for i in parameter_values:
    model = Ridge(alpha=i)
    model.fit(train_inputs, train_targets)
    train_error = rmse(model.predict(train_inputs), train_targets)
    val_error = rmse(model.predict(val_inputs), val_targets)
    train_errors.append(train_error)
    val_errors.append(val_error)
print('Minimum Error is ',min(val_errors),' is given by alpha =',parameter_values[val_errors.index(min(val_errors))])
#We will stop here with alpha=0.1471
#All the other default parameters are good to go with

#training with best parameters
model1 =  Ridge(alpha=0.1471, random_state=42)
model1.fit(train_inputs, train_targets)
tuned_train_preds1 = model1.predict(train_inputs)
tuned_val_preds1 = model1.predict(val_inputs)
rmse(tuned_train_preds1, train_targets), rmse(tuned_val_preds1, val_targets)

"""##### GradientBoosting Regressor
We will use `Grid Search Cross Validation` for `GradientBoostingRegressor` which checks all the possible combinations of the parameters given in the form of lists and gives us the best combination of parameters to make predictions on our dataset.
"""

model=GradientBoostingRegressor(random_state=42)
parameters = {
    'learning_rate':[0.01, 0.1, 0.2],
    'n_estimators':[200,240,260],
    'max_depth':[2,3,4],
    'max_features':[152,163,174]
}

#The lists of the parameters has been adjusted multiple times
#The bigger the list the longer it takes
#Initial parameters took me 16 hours, but thanks to kaggle it did all the work in background

from sklearn.model_selection import GridSearchCV
cv = GridSearchCV(model,parameters, cv=5)

#finding the best parameters
#seat back and relax cuz it's gonna take a while
cv.fit(inputs_df[numeric_cols + encoded_cols],log_targets.values.ravel())
cv.best_params_

#training with best parameters
model2 =  GradientBoostingRegressor(random_state=42,
                                   learning_rate=cv.best_params_['learning_rate'],
                                   max_depth=cv.best_params_['max_depth'],
                                   n_estimators=cv.best_params_['n_estimators'],
                                   max_features=cv.best_params_['max_features'])
model2.fit(train_inputs, train_targets)
tuned_train_preds2 = model2.predict(train_inputs)
tuned_val_preds2 = model2.predict(val_inputs)
rmse(tuned_train_preds2, train_targets), rmse(tuned_val_preds2, val_targets)

"""##### Let's compare the accuracy of the tuned models with that of untuned model"""

fig, axes = plt.subplots(2,2, figsize=(20, 10))

axes[0,0].set_title('Ridge')
sns.regplot(x=val_targets, y=val_preds1, ax=axes[0,0])
axes[0,0].set_ylabel('Predicted Sale Price (Untuned)')

axes[0,1].set_title('GradientBoostingRegressor')
sns.regplot(x=val_targets, y=val_preds2, ax=axes[0,1])
axes[0,1].set_ylabel('Predicted Sale Price (Untuned)')

sns.regplot(x=val_targets, y=tuned_val_preds1, ax=axes[1,0])
axes[1,0].set_ylabel('Predicted Sale Price (Tuned)')

sns.regplot(x=val_targets, y=tuned_val_preds2, ax=axes[1,1])
axes[1,1].set_ylabel('Predicted Sale Price (Tuned)');

"""The scenario seem to be improved here and we also got lesser RMSE for `GradientBoostingRegressor` than that of `Ridge()`. So rather than choosing one of these two models we will use both them for making the final predictions.

# 5. Weighted Average Ensemble
It's a machine learning approach that combines the predictions from multiple models, where the contribution of each model is weighted proportionally to its capability.

For assigning proper weight we will create a list of suitable weights and then loop through them again and again till we get the least RMSE possible with the combination of both models. (Just like we did while tuning the hyper-parameter for `Ridge()`.
"""

errors=[]
#weights=[i/10 for i in range(1,11)] Model1 weight:0.5, Model2 weight:0.5, RMSE:0.11474363067344863
#weights=[i/100 for i in range(40,61)] Model1 weight:0.45, Model2 weight:0.55, RMSE:0.11466126351179683
weights=[i/1000 for i in range(440,460)] # The Same "Model1 weight:0.45, Model2 weight:0.55, RMSE:0.11466126351179683" As Before
for i in weights:
    val_preds = i*tuned_val_preds1 + (1-i)*tuned_val_preds2
    errors.append(rmse(val_preds, val_targets))
j=weights[errors.index(min(errors))]
val_preds = j*tuned_val_preds1 + (1-j)*tuned_val_preds2
print('Model1 weight:{}, Model2 weight:{}, RMSE:{}'.format(j,1-j,rmse(val_preds, val_targets)))

import matplotlib.pyplot as plt
import seaborn as sns

val_preds = 0.45 * tuned_val_preds1 + 0.55 * tuned_val_preds2
fig, axes = plt.subplots(1, 3, figsize=(20, 5))
axes[0].set_title('Ridge and GradientBoostingRegressor')
axes[1].set_title('Ridge')
axes[2].set_title('GradientBoostingRegressor')

sns.regplot(x=val_targets, y=val_preds, ax=axes[0])
sns.regplot(x=val_targets, y=tuned_val_preds1, ax=axes[1])
sns.regplot(x=val_targets, y=tuned_val_preds2, ax=axes[2])

plt.show()

"""The data points are much closer to the regression line than they used to be for individually tuned models.

#### Feature Importance
"""

#Feature Importance Given by Ridge()
#We are using weights assigned to each attribute/feature by Ridge() as the importance given to the feature
weights=model1.coef_
weights_df = pd.DataFrame({
    'Feature': train_inputs.columns,
    'Importance': weights
}).sort_values('Importance', ascending=False)
plt.title('Feature Importance')
sns.barplot(data=weights_df.head(10), x='Importance', y='Feature');

#Feature Importance given by GradientBoostingRegressor()
#GradientBoostingRegressor() directly gives the feature importance
model2.feature_importances_
importance_df = pd.DataFrame({
    'Features': train_inputs.columns,
    'Importance': model2.feature_importances_
}).sort_values('Importance', ascending=False)
plt.title('Feature Importance')
sns.barplot(data=importance_df.head(10), x='Importance', y='Features');

"""You can clearly see that the feature importance assigned by `Ridge()` and `GradientBoostingRegressor()` is totally different than each other. In addition to that there are only 5 common features in the top 10. But the `Ridge()`'s plot is less skewed.

# 6. Making Predictions on Test Data

The combined models can be used to make predictions on new inputs using the following helper function:
"""

#fitting the model on the entire data
model1.fit(inputs_df[numeric_cols + encoded_cols], log_targets)
model2.fit(inputs_df[numeric_cols + encoded_cols], log_targets)

#this function does all the needful and makes the predictions on the inputs
def predict_input(input_df):
    input_df[log_numeric_cols]=np.log(input_df[log_numeric_cols]+1)
    input_df[numeric_cols] = imputer.transform(input_df[numeric_cols])
    input_df[numeric_cols] = scaler.transform(input_df[numeric_cols])
    input_df[encoded_cols] = encoder.transform(input_df[categorical_cols].values)
    X_input = input_df[numeric_cols + encoded_cols]
    prediction = 0.45*model1.predict(X_input) + 0.55*model2.predict(X_input)
    return np.exp(prediction)

#loading the test data
test=pd.read_csv('/content/test.csv')
test.head()

#loading the sample data
sample=pd.read_csv('/content/sample_submission.csv')

#making predictions on the test data and saving it for the submission
sample['SalePrice']=predict_input(test)
sample.head()

sample.to_csv('submission.csv',index=False)

"""# 7. Saving the model and objects

Let's save the model (along with other useful objects) to disk, so that we use it for making predictions without retraining.
"""

import joblib
house_price_predictor = {
    'model1': model1,
    'model2':model2,
    'imputer': imputer,
    'scaler': scaler,
    'encoder': encoder,
    'input_cols': input_cols,
    'target_col': target_col,
    'numeric_cols': numeric_cols,
    'categorical_cols': categorical_cols,
    'encoded_cols': encoded_cols,
    'log_numeric_cols':log_numeric_cols
}
joblib.dump(house_price_predictor, 'house_price_predictor.joblib')

"""# 8. Summary and References

<div class="alert alert-block alert-info" style='color:black;'>
    <h4>
        SummaryðŸ“Œ
    </h4>
    <b>
        I did this project for Machine Learning Internship of Progody Infotech where,
    </b>
    <br>
    1. Performed exploratory data analysis to make sense of the data and to observe its distribution and generate few insights.<br>
    2. Prepared the data for feeding it to the machine (program). The preparation included normalizing the data, filling the missing
    values, scaling the features and encoding the labels i.e. categorical attributes.<br>
    3. Trained, evaluated and tuned two models namely Ridge() and GradientBoostingRegressor().<br>
    4. Made predictions on the test data using Weighted Average Ensemble.<br>
    5. Submitted the project.
</div>

**About**\
This project is created by Irfan Ullah Khan. It is intended for educational and personal use.
"""